# 开题报告

面向数据中心网络的流量分析与故障检测

## 选题依据及意义

  数据中心被目前的企业中所广泛采用, 大型的数据中心甚至承载着公司的服务系统. 其中
服务器与服务器之间几乎完全是通过现代网络所连接. 对于庞大的系统来说, 各种问题均有
可能发生. 一旦出现, 就会影响着数据中心上承载的服务.

  面对大型复杂数据中心的调试以及错误, 如果仍然使用简陋的单机调试手段, 例如ping,
tracerout是无法满足运营要求的. 进而, 许多公司提出了自己的方案, 甚至为自己的集群
开发出一套相匹配的网络调试工具.

## 研究目标和主要内容(含论文(设计)提纲)

  在数据中心网络(DCN)中, 众多网络设备运行过程中的故障在所难免, 及时发现故障并确定
故障位置成为DCN网络运维的重要组成部分. 本课题参照微软提出的Everflow故障检测方案,
设计实现一款面向数据中心网络的流量分析工具.


## 文献综述: 国内外研究现状, 发展动态

  不论是学校还是企业, 许多都设立了自己的数据中心, 有关数据中心的网络故障检测, 都提出了自己的
见解与考量. 对于更为大型的数据中心网络, 更是有人提出了集学校公司之力, 共同开发系统, 不断的部署
与完善.

  经过对国内外近期文献的查阅, 我发现, 这些系统或是工具可以按照不同角度进行分类. 在主动性方面,
有主动探测故障的, 有被动触发, 还有使用日志记录来查看的; 在网络层次方面, 有些在数据包层面, 有些
则专注于数据流层面; 根据部署位置不同, 也可以分为在交换机部署, 在终端部署, 两者均进行部署; 在
使用范围上, 有些适合于通用平台, 有些只能适用于某个特定厂商; 在处理上, 有些拥有流水线来处理结果,
有些则是简单的排除错误; 按工具性质来看, 有些属于调试小工具, 专注于某个方面, 也不需要一直在线,
有些则是属于系统级别的服务工具, 为整个DCN所使用的服务, 需要一直在线; 而后在错误处理上, 有些
可以借助控制设备自动进行处理, 而有些则需要向管理员预警, 手动进行处理. 以下为各个方案的一些特点
介绍.

  Planck: \ref{rasley2014planck} SDN的出现使得自己调节的网络得以实现, 这样
的网络可以实时监控, 并能立即对重要的事件例如拥塞做出迅速反应. 但是目前的监控机制
需要数百毫秒来重新探测全局网络, 对于实时的错误, 这样的延迟是很致命的, 这篇2014年
的论文提出了新颖的网络测量架构, 使用端口镜像的机制来提取网络信息. 但是可以在相当
短的时间内对网络信息进行获取. 而且不会对网络造成太大的影响.

  LossRadar: \ref{li2016lossradar} 属于有针对性的工具, 着重介绍有关丢包的抓取
问题. 虽然只是检测丢包, 但是这也足够成为一个检测系统了, 这个工具可以在较快的时间
内抓取单独的丢包以及他们的详细信息. 他需要在交换机上部署, 但是并不需要很多的流量
和带宽. 也基于它开发了一些应用程序.

  Cherrypick: \ref{tammana2015cherrypick} 一个可扩展的简单的轨迹追踪技术 
目前的数据轨迹追踪需要负担大量数据积累的开销, 或是在数据平面大量资源的消耗,
交换机规则或是数据包头部的探查. 核心思想是挑选链接, 这些链接是表示数据包端到端
路径的关键, 并在到达目的地的路上将其嵌入数据包头部. 通过使用最新的头部标示技术,
它只需要很少的交换机规则即可, 

  SDN traceroute: \ref{agarwal2014sdn} 使用具有了SDN功能的设备, 不过只要
支持OpenFlow1.0的设备即可. 他可以通过SDN支持的网络中的任意数据包来确定路径,
这个路径使用SDN支持的转发机制, 而且并不需要修改转发规则. 可以探测任意的以太网
数据包的转发行为, 以及交换机和控制器逻辑中的调试问题.

  PathDump: \ref{tammana2016simplifying}使用了较为不同的方法, 仔细划分了边缘
设备与网络元素之间的调试任务. 利用边缘设备的资源进行网络调试的简约工具. 并且可以
支持大量网路调试问题. 需要的资源较少, 而且在较细的时间粒度调试.

  Pingmesh: \ref{guo2015pingmesh} 这个系统已经在微软的数据中心部署了超过4年,
他的理念很简单, 就是想要在任意时刻获取任意两台服务器的延迟信息. 因此, 他的目标就是
去定义一个网络延迟检测和分析系统, 他也需要为所有的服务器产生延迟信息, 因为延迟数据
属于基础信息, 能够帮助我们更好的管理网络, 以及解决网络中的问题. 这项服务也必须
长期在线, 并保证稳定性. 他可能是最不需要关心路由器等设备的一个方案了, 对于整个系统
来说, 知道延迟就是目标, 在实现时, 主要借助了ping的思想, 所有服务器要从中心控制器
下载文件, 进行延迟探测后传回中心服务器. 他借助了微软自己开发了存储系统, 也实现了
数据处理流水线. 相比于之后提出的Everflow, 他更像是微软的第一代产品, 稳定而强大.

  NetSight: \ref{handigol2014know}, 这是一个完全记录网络历史的工具.
在文章中, 介绍了如何使用packet histories(每个包的整条记录)来简化网络的调试.
为了展示`packet histories`的作用, 以及实现上的可行性, 创造了`NetSight`,
一个可扩展的平台, 允许程序简便的检索网络的历史状况. 在`NetSight`上, 有4个程序:
可交互的网络调试器, 实时的监视器, 一个历史记录器, 一个分级分析器.
在一个现代的多核服务器上, `NetSight`可以处理历史包在10G/s的链接中. 对于更大的
网络, 它可以通过增加服务器或是硬件 或是交换机的数量来扩展.
  需要借助SDN, 支持Openflow的交换机才能配合工作, 由于其记录了数据包的历史, 所以
能最大程度还原当初的网络模型, 也因此会给网络带来高负荷, 性能也会损失, 当然也可以
通过增加硬件来进行提升. 这是一个与Everflow较为相似的产品, Everflow中对其一个
重要的改进就是增加了匹配模式, 对特定的历史包生成记录, 能有效减少网络负载, 以及处理
难度.

  Netography: \ref{zhao2016netography} 这也是一种基于软件定义网络(SDN)的
一个工具, 之前的工作关注于静态检查, 被动监控, 以及活动探针, 这些依赖于控制设备以及网络
设备的抓取规则.
 他定义了一个数据包行为的概念, 用以描述数据包的真实变化, 并强调对故障排除的重要性. 基于通过
由主动发送的探测器触发的副本导出分组行为和流规则的新颖方法, 提出了Netography系统, 并说明了
关于转发错误时的排除任务过程, 以及由非租户争用造成的性能下降问题.

 Dissecting RTT(round trip time): \ref{marchetta2014dissecting} 与其说他
是工具, 不如认为这一种技术, 利用单个数据包探测延迟的技术. 研究人员
与操作者经常会在监控, 故障排除, 或是其他方式访问网络路径时 测量往返时间, 因为它结合
了所有跳的过程以及转发和反向路径, 很难去衡量特定网络元素的
延迟. 在这项工作中, 我们提出了一种新的方法: 在区块中,  映射特定路径后 基于单个数据包探测往返
延迟. 使用针对中间路由器的IP Prespecified Timestamp选项, 它可以提供慢速路径部分的往返时间
估计. 这个技术在Everflow中也被用到了, 用于探针方法测量时延.

  Everflow: \ref{greenberg2016packet} 这也是微软推出的工具, 属于数据包层面的
调试器, 该技术有两个优点, 仅需要具有"Match and Mirror"商用交换机即可, 而且可以
自定义的发送探针来进行测试. 也在微软的数据中心部署了超过6个月. 基于数据包层面的分析
可以有效获得整个系统的信息, 环路或是丢包等的常见问题有所探讨,  并且对于一些厂商
定制的协议, 例如RDMA流量的分析上, 也能具有很强大的分析功能.

  Passive Realtime Datacenter: \ref{roy2017passive} 这是在Facebook的数据
中心上进行测试的一种方案,  他不去简单的观察异常现象, 而是考虑异常对整个系统的性能
所造成的影响. 虽然是基于这么一种简单的设想. 但其实现需要深深结合数据中心的架构.
他们开发了轻量级的包标记技术, 仅仅使用转发规则(交换机支持). 作为路径中唯一标示.
文章中对于数据包头部, 现有IPv6头部还很难寻求一份区域用于调试.


## 方案论证.

  本次毕业设计基于

## 进度安排.


