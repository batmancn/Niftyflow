# 在大型数据中心网络中数据包级遥测

## Abstract

  在复杂网络中的错误调试经常需要在数据包级别的抓取和分析. 在这项任务中, 数据
中心由于规模, 流量以及错误的多样性而产生了独特的挑战. 为了能够及时的处理, DCN
的管理员必须 a) 辨识在大流量中被影响的包;  b) 在多个网络设备中追踪这些包; c) 
分析故障中的流量路径; d) 测试或是确认潜在的问题. 据我们所知, 目前没有工具可以
达到这项任务所需的特异性和规模.

  我们推出了Everflow, 一个针对数据中心网络的数据包级别的网络遥测系统. Everflow
可以追踪特定的包, 通过实现了一个包过滤器, 他基于商用交换机的"匹配复制"机制.
他将捕获到的包重新分发到不同的分析服务器上(使用交换机内置的ASIC芯片的负载均衡),
他发送指南探针去确认潜在的错误. 我们目前的实验演示了Everflow的扩展性, 并且分享
了近6个月以来的解决的问题, 这些问题来源于运行环境.

## Introduction

  从在线商业到智能设备, 数据中心网络是大型在线服务的核心. DCNs被大量的采用, 即使
是性能上的一点降低或是故障引起的超时都会造成数以百万计的损失. 以上这些要求我们
建立主动管理DCN的模型, 其中的设备可以观察, 分析, 并且及时纠正错误.

  理解和调试在DCN中的错误是很有挑战的, 因为错误有各式各样, 例如:
    1. A=>B服务器中的一些包可能会有很高的延迟, 但是并不清楚是哪一个链接该负责
    2. 发往特定的一组服务器的数据包可能会被丢失, 即使丢包计时器没有表现异常
    3. 到虚拟IP的TCP连接可能会有断断续续的超时, 并且traceroute探测去调试时也会被负载均衡所阻塞
    4. 由于一组ECMP的连接而导致负载未均衡, 网络管理者并不知道问题是由流量大小的差异还是更深的原因

  这些问题的诊断需要我们在包的粒度上进行测试. 由于不完善的接口或是交换机的软件故障
可能产生随机的错误, 并且影响一组具有特定特征的包, 这些特征可以是采取的路线,
包头部或是时间. 由于其微妙的关系, 对于这些错误, 通过分析网络流信息, 聚合统计
或是抽样的流量也很难调试. 我们转而追踪, 收集分析数据包级别的流量行为, 作为
数据包级别的遥测系统.

  建立可靠的包级别的探测系统有三个挑战, 首先, 当前的SCN具有史无前例的网络流量
一个庞大的DCN拥有超过10万个服务器, 每个都有10-40Gbps的网络流量. 在高利用率下
总流量很容易超过100Tbps. 对于今天的商用交换机来说, 分析其中的一部分流量都是棘手
的, 并且将流量移动到商用的服务器中分析会导致网络拥塞, 甚至破坏网络.

  其次, DCN的错误经常会发生在多跳或是多个交换机中, 并且有效的追踪需要智能的追溯
网络中的一小部分包, 以及基于复杂的匹配模式去查找, 例如协议头, 起始点, 甚至是
路径中的设备. 这个过程不只是大海捞针, 还是要找到特定大小, 形状, 以颜色的针.
目前存在的基于包级别的分析是不分青红皂白的追踪数据包, 对于大规模的DCN 或是复杂
的匹配模式, 他们也不能提供扩展性.

  而后, 被动的追踪瞬间的网络快照, 将会被性能所限制. 快照中的路径可能不足以判断
问题是暂时的或是永久的. 也不能提供足够的信息去定位错误. 例如, 我们看到一组包
在到达终点时停止, 我们无法判断出这是由于随机丢弃或是黑洞.

  我们推出了Everflow, 一个网络检测系统 提供了高扩展性以及弹性的访问数据包层面
的信息, 与追踪单独的包不同的是, Everflow使用了"匹配复制"机制. 商用的交换机可以
灵活的匹配包的头部或是负载, 我们使用这一特性, 为我们的分析服务器镜像数据包
为交换机安装一组精心选定的匹配镜像规则, 我们可以既进行数据包级别的追踪,
并且将开销降低几个数量级. 为了快速的在大量的数据包路径中搜索, 我们建造了一个
可扩展的收集与分析流水线. 我们使用基于交换机的负载均衡功能来分布多个服务器上的
跟踪和处理, 同时保持流级别的局部性以进行有效的分析. 最后, Everflow允许探测包 --
特制的注入到网络中的数据包, 遵循预设的路径. 探测器可以验证单个设备的行为和性能

  Everflow 已经在2014年8月在微软的DCN上部分部署了, 包括有37太交换机的小集群以及
拥有440台交换机的大集群中. 这些集群承载了巨大的应用流量. 我们也有选择的
在其他生产集群中部署Everflow来追踪网络错误. 我们抓取一些部署经历, 通过描述几个
有代表性的调试案例(在第7章), 包括延迟问题, 包丢失, 路由环路, ECMP负载不均衡,
以及RDMA协议上的特定问题. 在每个例子中, 我们描述了可观察到的症状, 所用的步骤,
以及如何使用Everflow得到结果.

  我们执行详细的微基准来量化Everflow在关键方面的表现, 包括数据包分析速率,
带宽存储开销, 以及整体系统的扩展性. 我们的评估一致表明, Everflow的所有组件
需要了较低的开销, 并且能够很好的适应大型的DCN和100Tbps的流量.


  在开发Everflow时,我们试图解决对可扩展的数据包级遥测的真正需求,这种遥测可
以调试难以通过传统技术解决的故障. 我们优先于功能或功能的可用性和简单性. Everflow
工作在商用交换机上而且不需要其他特殊的硬件, 我们在Everflow上的工作表明他对
大型DCN的支持, 并且为网络管理员提供了巨大的好处.

## Packet-Lever network telemetry

  运营DCN包括各种各样的硬件和软件组件, 包括多种类型的交换机, 负载均衡器和专用
于处理和存储的服务器. 错误可能会由于单个或是几种组件所造成, 这使得调试变得十分
困难. 在这一章, 我们描述了在一些发生在我们大型DCN上的错误例子, 用以显示 传统
工具的局限以及数据包级别检测的优势.

  **静默的丢包**: 罪魁祸首的交换机并不报告(尽管计数器为0). 这种状况可能是由于
交换机上的软件bug或是硬件错误, 即使在发送端检测到了TCP报文的重发, 但也会因为交换机
较多也无法确定是那个交换机出现了问题. 通过连续的在交换机中检测特定数据包, 我们
可以立刻发现最后一跳交换机, 以及被丢失的数据包期待的下一跳的交换机. 然后我们发送
探针至下一跳的交换机中来确定罪魁祸首.

  **静默的黑洞**: 路由黑洞不在转发表中显示. 因此, 他不能被检查转发表的工具所检测到.
静默的黑洞可能由于损坏的TCAM表条目而发生. 同静默的丢包一样, 我们也可以检测和定位到
静默的黑洞. 

  **虚增的端到端延迟**: 一个流量的高延迟, 很容易被终端主机所检测, 但很难利用传统
工具进行调试. 但是通过包级别的追踪, 这个问题变得不重要, 我们可以获得每一跳的延迟

  **从错误的中间件路由中循环**: 路由的问题由中间件产生而不是由交换机产生. 这可能
是由于中间件错误的修改了包结构以及转发行为. 这样的问题不能通过检测交换机路由或是
转发表来解决. 给定一组数据包, 我们可以轻松的得知这样的问题, 因为该路径将会违反
基本的原则例如无环

  **负载均衡**: 流量由一组ECMP链路不均匀的传递. 原始的通过ECMP的连接数进行的
检测很有可能误报, 因为链路的负载可能由不同大小的流量来决定. 即使检测到负载不均衡,
计数器只能粗粒度(coarse-gained)的在调试中回答问题, 只能回答"这种不均衡是由符合
特定模式的流量组成的吗?"这样的问题. 但是, 一个数据包级别的检测允许我们检测到映射
在每条链路上的[5元组模型][5-tuple], 并且更加可靠.

  **协议Bug**: 在实现BGP, PFC(优先级流量控制), RDMA(远程直接内存访问)的协议时
可能会有bug. 当协议行为异常时, 网络的性能和可性就受到了影响. 解决协议的问题是很有
挑战的. 因为许多协议可能由第三方供应商(交换机和网卡)实现, 并且也不容易被仪器探测.
追踪这些协议的数据包提供了一种可靠的方案来检测协议缺陷. 这里, 网络遥测系统是十分
适合的, 因为基于主机的追踪系统昂贵(对于RDMA), 不可达(PFC和BGP).

## Overview of everflow

  这一章介绍了在系统扩展性上的挑战, 并且介绍了解决挑战的关键点.

###  Design challenges

**可扩展的分析与追踪**. 面向大型DCN的遥测系统, 第一个挑战就是路径追踪的扩展性.
就像先前提到的一样, 大型DCN(10W以上的服务器)中流量轻松就能达到100Tbps, 追踪这样
规模的数据包需要大量的网络资源与服务器资源, 想象一种典型的情况, 平均每个包的大小为
1000bytes, 每个镜像数据包64bytes(只包括头部信息) 并且网络的直径为5跳. 如果我们
简单的追踪每个数据包的每一跳作为数据包历史, 数据流量就会有这么多
$\frac{64B}{1000B} \times 5(hops) \times 1000(Tbps) = 32(Tbps)$, 如此高的追踪流量
将会造成网络拥塞和丢包, 特别是当网络利用率很高时.

  另一个扩展性的挑战是路径分析. 因为商用交换机有限的存储与CPU运算能力, 追踪数据包
必须由服务器来完成. 即使假设一个服务可以处理10Gbps的流量, 我们需要
$\frac{32Tbps}{10Tbps} = 3200$这么多服务器来处理, 这也是很大的开销.

  天真的做起来, 情况会更糟. 对于大部分的分期, 同一路径每一跳的数据包应该被发送到
相同的分析服务器中. 这个要求可以通过向一些洗牌器发送数据包来解决, 而后根据数据包
的头部hash值将数据包改组, 增加洗牌的服务器将会需要增加一倍的服务器.

**被动追踪的限制**. 在现实中, 被动的网络追踪可能不能提供解决问题的有效信息. 图一
显示了一种情况. TODO: 图介绍


### 3.2 Key ideas

 Everflow解决上述问题有四个关键点, 前三个是为了解决扩展性, 最后一个是未来解决
被动追踪的限制.

**在交换机上的匹配及镜像**. 商用交换机可以基于事先订立的规则来匹配, 然后执行某些
操作(镜像和封装), 而且这并不影响原有数据包的转发行为.Everflow使用这一点来减少
追踪开销. 特别的是, 我们设计了三种类型的匹配规则去解决DCN的一般错误. 这个规则绝不
是详尽的, 可以通过扩展规则来应对其他类型的错误.

  首先, 我们设计匹配规则去抓取每一个数据流, 最直接的方法是随机的抓取n个包中的一个,
这是严重偏向大流量的. 在DCN中, 数据流的大小分布是十分不均的, 这样就会错过了许多
小流量. 而那些小流量通常是面向用户的, 交互服务是有严格的性能要求的. 为了将小流量
覆盖到, 我们建立了一条新的规则, 这套新的规则基于TCP的SYN, FIN, RST数据包区域,
因为DCN的流量大部分由TCP表示, 这个规则允许我们追踪DCN中的每个TCP流量.

  而后, 我们配置额外的匹配规则可以灵活的追踪. 基础的TCP匹配不能抓取每一种类型的
错误, 例如, TCP流中部的数据包丢失. 例如, 我们可能想要追踪特定程序的数据包, 特定
的端口, 或是在某些服务器对之间. 为了允许可扩展的追踪, 我们允许数据包头部有一位被
标记为debug. 这个标记标准可以以任何方式定义, 只要全部的追踪影响低于一个阈值. 在
交换机中, 我们安装了一条规则去追踪任何有debug位的数据包, 就像软件开发者在编译期间
开启debug标志来启用细粒度的追踪, 因此, Everflow可以有效追踪常规数据包的任何子集.

  最后, 我们的追踪不只覆盖数据包. 一个DCN中有一小部分流量与网络协议相关, 例如
BGP, PFC, RDMA的流量包. 我们将其称为协议流量用以区分正常的数据流量. 尽管纯粹的
协议流量是很小的, 但却与DCN的健康相关. 因此, Everflow的规则中追踪所有协议流量.

**可扩展的跟踪分析器**. 尽管匹配规则限制了一些追踪开销, 但是因为DCN的规模庞大,
全部的追踪流量还是可能会很大. 为了减少这样的分析开销, 我们观察到, 在任何时候, 只有
一小部分被追踪的数据包(<0.01%)会出现异常行为(例如循环或丢弃). 这一现象的驱动我们
将正常包与异常包区别来对待, 例如, 前者保留详细记录每一跳的状态, 而后者的每个设备
状态, 我们在4.1章描述, 这样不同的对待使得我们减少了分析开销(三个数量级). 如果需要,
我们可以有选择的恢复丢失的信息, 只要通过探针追踪即可(如下所诉).

**基于交换机的洗牌器**. 就像在3.1中描述的一样, 我们需要一个开销很低的方式去将追踪
流量改组. 我们使用了先前所做的工作, 将商用交换机转变为一个硬件复用器来负载均衡.
首先在复用器中中定义一个虚拟VIP,  他将映射到一组`DIP`, 每个DIP将会对应一个分析
服务器. 我们之后将会改变所有的交换器配置, 使他们向VIP发送数据包, 当数据包`p`到达
复用器时, 复用器会根据5元组重定向数据包, 到特定的`DIP`中. 这就保证了具有相同
五元组的追踪数据包将会重定向到相同的DIP中.

  一个复用器可以利用交换机的能力来重定向流量(> 1Tbps). 这要比普通10Gbps的服务器
网卡快至少100倍, 并且极大的减少了重定向的开销, 而且可以通过增加相同VIP的复用器
来提高转换能力.

  我们需要注意封装后的数据包. (目前为止, 我们先忽略掉追踪数据包也被分装这一现象,
这个将会在6.1章讨论). 包封装经常会用在DCN的负载均衡以及网络虚拟化中. 图三TODO: 图问题
举了一个例子, 软件负载均衡器如可以打破追踪分析. 原始的数据包p的目的IP是 `VIP`(
复用器的IP), 一个复用器可以使用新的`DIP`(分析服务器)作为数据包p的新的目的IP.
这样做, 原始的数据包p和封装后的数据包p将会被发送至不同的分析器中, 也会被单独的处理.
为了避免这个错误, 我们安装了一个规则去匹配封装数据包的内层头部, 并且也配置复用器
对内层头部进行hash操作. 这允许我们追中数据包p的完整路径.

**有针对性的探测**. 像先前提出的那样, 丢包可能是由于多种原因发生的. 有时, 只进行
被动的追踪可能不会有效区分不同的可能性. 这样的歧义产生了如下的问题: 如果我们可以
任意的重放数据包路径呢? 更直接的是, 如果我们可以注入任何想要构造的数据包到任意的
交换机中, 并且追踪被注入的数据包(设置debug位)的行为, 我们称之位探针, 将会在6.2
章进行详细套路.

  一个有效应用就是根据采样或是聚合的数据恢复路径信息. 为了恢复任何数据包p的路径,
我们只需要将数据包p注入到第一跳的交换机中, 当然设置了debug位.

  进一步来讲, 探针克服了被动追踪时的限制. 在图1的例子中TODO: 图, 我们可以注入
许多数据包p的被封到交换机S2中, 去探测丢包是否是永久的. 另外, 我们也可以使用不同
的5元组模式来构造探针数据包, 以此来看看丢包是否是针对特定的5元组. 这样的探针不能
调试临时错误, 因为他们可能在探针初始化前就消失了. 我们认为这样是可以接受的, 因为
永久的错误要比临时错误更有影响.

  我们也扩展了探针, 使其不止能注入数据包至合适的交换机, 也能创造数据包并让其游历
我们期望的路径. 这个扩展使得我们可以衡量网络中任意链接的延迟. 就像在图2TODO: 图
一个探针p可被要求遍历S1->S2->S1, 因为p穿过了$S_{1}$两次, $S_{1}$就能产生两个
数据包p的时间$t_{1}$和$t_{2}$, 并且$t_{2} - t_{1}$就等于$S_{1}, $S_{2}$的
往返延迟.

  因为目前的许多交换机不提供时间戳函数, 我们不能直接的获取$t_{1}$, $t_{2}$.
然而, 两个数据包p在时间上是接近的, 并且也要经过相同的路径达到分析器. 因此, 我们
计算他们到达分析器的不同的时间之差约等于$t_{2} - t_{1}$.










