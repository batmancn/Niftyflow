# 在大型数据中心网络中数据包级遥测

## Abstract

  在复杂网络中的错误调试经常需要在数据包级别的抓取和分析. 在这项任务中, 数据
中心由于规模, 流量以及错误的多样性而产生了独特的挑战. 为了能够及时的处理, DCN
的管理员必须 a) 辨识在大流量中被影响的包;  b) 在多个网络设备中追踪这些包; c) 
分析故障中的流量路径; d) 测试或是确认潜在的问题. 据我们所知, 目前没有工具可以
达到这项任务所需的特异性和规模.

  我们推出了Everflow, 一个针对数据中心网络的数据包级别的网络遥测系统. Everflow
可以追踪特定的包, 通过实现了一个包过滤器, 他基于商用交换机的"匹配复制"机制.
他将捕获到的包重新分发到不同的分析服务器上(使用交换机内置的ASIC芯片的负载均衡),
他发送指南探针去确认潜在的错误. 我们目前的实验演示了Everflow的扩展性, 并且分享
了近6个月以来的解决的问题, 这些问题来源于运行环境.

## Introduction

  从在线商业到智能设备, 数据中心网络是大型在线服务的核心. DCNs被大量的采用, 即使
是性能上的一点降低或是故障引起的超时都会造成数以百万计的损失. 以上这些要求我们
建立主动管理DCN的模型, 其中的设备可以观察, 分析, 并且及时纠正错误.

  理解和调试在DCN中的错误是很有挑战的, 因为错误有各式各样, 例如:
    1. A=>B服务器中的一些包可能会有很高的延迟, 但是并不清楚是哪一个链接该负责
    2. 发往特定的一组服务器的数据包可能会被丢失, 即使丢包计时器没有表现异常
    3. 到虚拟IP的TCP连接可能会有断断续续的超时, 并且traceroute探测去调试时也会被负载均衡所阻塞
    4. 由于一组ECMP的连接而导致负载未均衡, 网络管理者并不知道问题是由流量大小的差异还是更深的原因

  这些问题的诊断需要我们在包的粒度上进行测试. 由于不完善的接口或是交换机的软件故障
可能产生随机的错误, 并且影响一组具有特定特征的包, 这些特征可以是采取的路线,
包头部或是时间. 由于其微妙的关系, 对于这些错误, 通过分析网络流信息, 聚合统计
或是抽样的流量也很难调试. 我们转而追踪, 收集分析数据包级别的流量行为, 作为
数据包级别的遥测系统.

  建立可靠的包级别的探测系统有三个挑战, 首先, 当前的SCN具有史无前例的网络流量
一个庞大的DCN拥有超过10万个服务器, 每个都有10-40Gbps的网络流量. 在高利用率下
总流量很容易超过100Tbps. 对于今天的商用交换机来说, 分析其中的一部分流量都是棘手
的, 并且将流量移动到商用的服务器中分析会导致网络拥塞, 甚至破坏网络.

  其次, DCN的错误经常会发生在多跳或是多个交换机中, 并且有效的追踪需要智能的追溯
网络中的一小部分包, 以及基于复杂的匹配模式去查找, 例如协议头, 起始点, 甚至是
路径中的设备. 这个过程不只是大海捞针, 还是要找到特定大小, 形状, 以颜色的针.
目前存在的基于包级别的分析是不分青红皂白的追踪数据包, 对于大规模的DCN 或是复杂
的匹配模式, 他们也不能提供扩展性.

  而后, 被动的追踪瞬间的网络快照, 将会被性能所限制. 快照中的路径可能不足以判断
问题是暂时的或是永久的. 也不能提供足够的信息去定位错误. 例如, 我们看到一组包
在到达终点时停止, 我们无法判断出这是由于随机丢弃或是黑洞.

  我们推出了Everflow, 一个网络检测系统 提供了高扩展性以及弹性的访问数据包层面
的信息, 与追踪单独的包不同的是, Everflow使用了"匹配复制"机制. 商用的交换机可以
灵活的匹配包的头部或是负载, 我们使用这一特性, 为我们的分析服务器镜像数据包
为交换机安装一组精心选定的匹配镜像规则, 我们可以既进行数据包级别的追踪,
并且将开销降低几个数量级. 为了快速的在大量的数据包路径中搜索, 我们建造了一个
可扩展的收集与分析流水线. 我们使用基于交换机的负载均衡功能来分布多个服务器上的
跟踪和处理, 同时保持流级别的局部性以进行有效的分析. 最后, Everflow允许探测包 --
特制的注入到网络中的数据包, 遵循预设的路径. 探测器可以验证单个设备的行为和性能

  Everflow 已经在2014年8月在微软的DCN上部分部署了, 包括有37太交换机的小集群以及
拥有440台交换机的大集群中. 这些集群承载了巨大的应用流量. 我们也有选择的
在其他生产集群中部署Everflow来追踪网络错误. 我们抓取一些部署经历, 通过描述几个
有代表性的调试案例(在第7章), 包括延迟问题, 包丢失, 路由环路, ECMP负载不均衡,
以及RDMA协议上的特定问题. 在每个例子中, 我们描述了可观察到的症状, 所用的步骤,
以及如何使用Everflow得到结果.

  我们执行详细的微基准来量化Everflow在关键方面的表现, 包括数据包分析速率,
带宽存储开销, 以及整体系统的扩展性. 我们的评估一致表明, Everflow的所有组件
需要了较低的开销, 并且能够很好的适应大型的DCN和100Tbps的流量.


  在开发Everflow时,我们试图解决对可扩展的数据包级遥测的真正需求,这种遥测可
以调试难以通过传统技术解决的故障. 我们优先于功能或功能的可用性和简单性. Everflow
工作在商用交换机上而且不需要其他特殊的硬件, 我们在Everflow上的工作表明他对
大型DCN的支持, 并且为网络管理员提供了巨大的好处.

## Packet-Lever network telemetry

  运营DCN包括各种各样的硬件和软件组件, 包括多种类型的交换机, 负载均衡器和专用
于处理和存储的服务器. 错误可能会由于单个或是几种组件所造成, 这使得调试变得十分
困难. 在这一章, 我们描述了在一些发生在我们大型DCN上的错误例子, 用以显示 传统
工具的局限以及数据包级别检测的优势.

  **静默的丢包**: 罪魁祸首的交换机并不报告(尽管计数器为0). 这种状况可能是由于
交换机上的软件bug或是硬件错误, 即使在发送端检测到了TCP报文的重发, 但也会因为交换机
较多也无法确定是那个交换机出现了问题. 通过连续的在交换机中检测特定数据包, 我们
可以立刻发现最后一跳交换机, 以及被丢失的数据包期待的下一跳的交换机. 然后我们发送
探针至下一跳的交换机中来确定罪魁祸首.

  **静默的黑洞**: 路由黑洞不在转发表中显示. 因此, 他不能被检查转发表的工具所检测到.
静默的黑洞可能由于损坏的TCAM表条目而发生. 同静默的丢包一样, 我们也可以检测和定位到
静默的黑洞. 

  **虚增的端到端延迟**: 一个流量的高延迟, 很容易被终端主机所检测, 但很难利用传统
工具进行调试. 但是通过包级别的追踪, 这个问题变得不重要, 我们可以获得每一跳的延迟

  **从错误的中间件路由中循环**: 路由的问题由中间件产生而不是由交换机产生. 这可能
是由于中间件错误的修改了包结构以及转发行为. 这样的问题不能通过检测交换机路由或是
转发表来解决. 给定一组数据包, 我们可以轻松的得知这样的问题, 因为该路径将会违反
基本的原则例如无环

  **负载均衡**: 流量由一组ECMP链路不均匀的传递. 原始的通过ECMP的连接数进行的
检测很有可能误报, 因为链路的负载可能由不同大小的流量来决定. 即使检测到负载不均衡,
计数器只能粗粒度(coarse-gained)的在调试中回答问题, 只能回答"这种不均衡是由符合
特定模式的流量组成的吗?"这样的问题. 但是, 一个数据包级别的检测允许我们检测到映射
在每条链路上的[5元组模型][5-tuple], 并且更加可靠.

  **协议Bug**: 在实现BGP, PFC(优先级流量控制), RDMA(远程直接内存访问)的协议时
可能会有bug. 当协议行为异常时, 网络的性能和可性就受到了影响. 解决协议的问题是很有
挑战的. 因为许多协议可能由第三方供应商(交换机和网卡)实现, 并且也不容易被仪器探测.
追踪这些协议的数据包提供了一种可靠的方案来检测协议缺陷. 这里, 网络遥测系统是十分
适合的, 因为基于主机的追踪系统昂贵(对于RDMA), 不可达(PFC和BGP).

## Overview of everflow

  这一章介绍了在系统扩展性上的挑战, 并且介绍了解决挑战的关键点.

###  Design challenges

**可扩展的分析与追踪**. 面向大型DCN的遥测系统, 第一个挑战就是路径追踪的扩展性.
就像先前提到的一样, 大型DCN(10W以上的服务器)中流量轻松就能达到100Tbps, 追踪这样
规模的数据包需要大量的网络资源与服务器资源, 想象一种典型的情况, 平均每个包的大小为
1000bytes, 每个镜像数据包64bytes(只包括头部信息) 并且网络的直径为5跳. 如果我们
简单的追踪每个数据包的每一跳作为数据包历史, 数据流量就会有这么多
$\frac{64B}{1000B} \times 5(hops) \times 1000(Tbps) = 32(Tbps)$, 如此高的追踪流量
将会造成网络拥塞和丢包, 特别是当网络利用率很高时.









