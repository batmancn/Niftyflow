---
title: 在大型数据中心网络中数据包级遥测(翻译)

---

## Abstract

  在复杂网络中的错误调试经常需要在数据包级别的抓取和分析. 在这项任务中, 数据
中心由于规模, 流量以及错误的多样性而产生了独特的挑战. 为了能够及时的处理, DCN
的管理员必须 a) 辨识在大流量中被影响的包;  b) 在多个网络设备中追踪这些包; c) 
分析故障中的流量路径; d) 测试或是确认潜在的问题. 据我们所知, 目前没有工具可以
达到这项任务所需的特异性和规模.

  我们推出了Everflow, 一个针对数据中心网络的数据包级别的网络遥测系统. Everflow
可以追踪特定的包, 通过实现了一个包过滤器, 他基于商用交换机的"匹配复制"机制.
他将捕获到的包重新分发到不同的分析服务器上(使用交换机内置的ASIC芯片的负载均衡),
他发送指南探针去确认潜在的错误. 我们目前的实验演示了Everflow的扩展性, 并且分享
了近6个月以来的解决的问题, 这些问题来源于运行环境.

## Introduction

  从在线商业到智能设备, 数据中心网络是大型在线服务的核心. DCNs被大量的采用, 即使
是性能上的一点降低或是故障引起的超时都会造成数以百万计的损失. 以上这些要求我们
建立主动管理DCN的模型, 其中的设备可以观察, 分析, 并且及时纠正错误.

  理解和调试在DCN中的错误是很有挑战的, 因为错误有各式各样, 例如:
    1. A=>B服务器中的一些包可能会有很高的延迟, 但是并不清楚是哪一个链接该负责
    2. 发往特定的一组服务器的数据包可能会被丢失, 即使丢包计时器没有表现异常
    3. 到虚拟IP的TCP连接可能会有断断续续的超时, 并且traceroute探测去调试时也会被负载均衡所阻塞
    4. 由于一组ECMP的连接而导致负载未均衡, 网络管理者并不知道问题是由流量大小的差异还是更深的原因

  这些问题的诊断需要我们在包的粒度上进行测试. 由于不完善的接口或是交换机的软件故障
可能产生随机的错误, 并且影响一组具有特定特征的包, 这些特征可以是采取的路线,
包头部或是时间. 由于其微妙的关系, 对于这些错误, 通过分析网络流信息, 聚合统计
或是抽样的流量也很难调试. 我们转而追踪, 收集分析数据包级别的流量行为, 作为
数据包级别的遥测系统.

  建立可靠的包级别的探测系统有三个挑战, 首先, 当前的SCN具有史无前例的网络流量
一个庞大的DCN拥有超过10万个服务器, 每个都有10-40Gbps的网络流量. 在高利用率下
总流量很容易超过100Tbps. 对于今天的商用交换机来说, 分析其中的一部分流量都是棘手
的, 并且将流量移动到商用的服务器中分析会导致网络拥塞, 甚至破坏网络.

  其次, DCN的错误经常会发生在多跳或是多个交换机中, 并且有效的追踪需要智能的追溯
网络中的一小部分包, 以及基于复杂的匹配模式去查找, 例如协议头, 起始点, 甚至是
路径中的设备. 这个过程不只是大海捞针, 还是要找到特定大小, 形状, 以颜色的针.
目前存在的基于包级别的分析是不分青红皂白的追踪数据包, 对于大规模的DCN 或是复杂
的匹配模式, 他们也不能提供扩展性.

  而后, 被动的追踪瞬间的网络快照, 将会被性能所限制. 快照中的路径可能不足以判断
问题是暂时的或是永久的. 也不能提供足够的信息去定位错误. 例如, 我们看到一组包
在到达终点时停止, 我们无法判断出这是由于随机丢弃或是黑洞.

  我们推出了Everflow, 一个网络检测系统 提供了高扩展性以及弹性的访问数据包层面
的信息, 与追踪单独的包不同的是, Everflow使用了"匹配复制"机制. 商用的交换机可以
灵活的匹配包的头部或是负载, 我们使用这一特性, 为我们的分析服务器镜像数据包
为交换机安装一组精心选定的匹配镜像规则, 我们可以既进行数据包级别的追踪,
并且将开销降低几个数量级. 为了快速的在大量的数据包路径中搜索, 我们建造了一个
可扩展的收集与分析流水线. 我们使用基于交换机的负载均衡功能来分布多个服务器上的
跟踪和处理, 同时保持流级别的局部性以进行有效的分析. 最后, Everflow允许探测包 --
特制的注入到网络中的数据包, 遵循预设的路径. 探测器可以验证单个设备的行为和性能

  Everflow 已经在2014年8月在微软的DCN上部分部署了, 包括有37太交换机的小集群以及
拥有440台交换机的大集群中. 这些集群承载了巨大的应用流量. 我们也有选择的
在其他生产集群中部署Everflow来追踪网络错误. 我们抓取一些部署经历, 通过描述几个
有代表性的调试案例(在第7章), 包括延迟问题, 包丢失, 路由环路, ECMP负载不均衡,
以及RDMA协议上的特定问题. 在每个例子中, 我们描述了可观察到的症状, 所用的步骤,
以及如何使用Everflow得到结果.

  我们执行详细的微基准来量化Everflow在关键方面的表现, 包括数据包分析速率,
带宽存储开销, 以及整体系统的扩展性. 我们的评估一致表明, Everflow的所有组件
需要了较低的开销, 并且能够很好的适应大型的DCN和100Tbps的流量.


  在开发Everflow时,我们试图解决对可扩展的数据包级遥测的真正需求,这种遥测可
以调试难以通过传统技术解决的故障. 我们优先于功能或功能的可用性和简单性. Everflow
工作在商用交换机上而且不需要其他特殊的硬件, 我们在Everflow上的工作表明他对
大型DCN的支持, 并且为网络管理员提供了巨大的好处.

## Packet-Lever network telemetry

  运营DCN包括各种各样的硬件和软件组件, 包括多种类型的交换机, 负载均衡器和专用
于处理和存储的服务器. 错误可能会由于单个或是几种组件所造成, 这使得调试变得十分
困难. 在这一章, 我们描述了在一些发生在我们大型DCN上的错误例子, 用以显示 传统
工具的局限以及数据包级别检测的优势.

  **静默的丢包**: 罪魁祸首的交换机并不报告(尽管计数器为0). 这种状况可能是由于
交换机上的软件bug或是硬件错误, 即使在发送端检测到了TCP报文的重发, 但也会因为交换机
较多也无法确定是那个交换机出现了问题. 通过连续的在交换机中检测特定数据包, 我们
可以立刻发现最后一跳交换机, 以及被丢失的数据包期待的下一跳的交换机. 然后我们发送
探针至下一跳的交换机中来确定罪魁祸首.

  **静默的黑洞**: 路由黑洞不在转发表中显示. 因此, 他不能被检查转发表的工具所检测到.
静默的黑洞可能由于损坏的TCAM表条目而发生. 同静默的丢包一样, 我们也可以检测和定位到
静默的黑洞. 

  **虚增的端到端延迟**: 一个流量的高延迟, 很容易被终端主机所检测, 但很难利用传统
工具进行调试. 但是通过包级别的追踪, 这个问题变得不重要, 我们可以获得每一跳的延迟

  **从错误的中间件路由中循环**: 路由的问题由中间件产生而不是由交换机产生. 这可能
是由于中间件错误的修改了包结构以及转发行为. 这样的问题不能通过检测交换机路由或是
转发表来解决. 给定一组数据包, 我们可以轻松的得知这样的问题, 因为该路径将会违反
基本的原则例如无环

  **负载均衡**: 流量由一组ECMP链路不均匀的传递. 原始的通过ECMP的连接数进行的
检测很有可能误报, 因为链路的负载可能由不同大小的流量来决定. 即使检测到负载不均衡,
计数器只能粗粒度(coarse-gained)的在调试中回答问题, 只能回答"这种不均衡是由符合
特定模式的流量组成的吗?"这样的问题. 但是, 一个数据包级别的检测允许我们检测到映射
在每条链路上的[5元组模型][5-tuple], 并且更加可靠.

  **协议Bug**: 在实现BGP, PFC(优先级流量控制), RDMA(远程直接内存访问)的协议时
可能会有bug. 当协议行为异常时, 网络的性能和可性就受到了影响. 解决协议的问题是很有
挑战的. 因为许多协议可能由第三方供应商(交换机和网卡)实现, 并且也不容易被仪器探测.
追踪这些协议的数据包提供了一种可靠的方案来检测协议缺陷. 这里, 网络遥测系统是十分
适合的, 因为基于主机的追踪系统昂贵(对于RDMA), 不可达(PFC和BGP).

## Overview of everflow

  这一章介绍了在系统扩展性上的挑战, 并且介绍了解决挑战的关键点.

###  Design challenges

**可扩展的分析与追踪**. 面向大型DCN的遥测系统, 第一个挑战就是路径追踪的扩展性.
就像先前提到的一样, 大型DCN(10W以上的服务器)中流量轻松就能达到100Tbps, 追踪这样
规模的数据包需要大量的网络资源与服务器资源, 想象一种典型的情况, 平均每个包的大小为
1000bytes, 每个镜像数据包64bytes(只包括头部信息) 并且网络的直径为5跳. 如果我们
简单的追踪每个数据包的每一跳作为数据包历史, 数据流量就会有这么多
$\frac{64B}{1000B} \times 5(hops) \times 1000(Tbps) = 32(Tbps)$, 如此高的追踪流量
将会造成网络拥塞和丢包, 特别是当网络利用率很高时.

  另一个扩展性的挑战是路径分析. 因为商用交换机有限的存储与CPU运算能力, 追踪数据包
必须由服务器来完成. 即使假设一个服务可以处理10Gbps的流量, 我们需要
$\frac{32Tbps}{10Tbps} = 3200$这么多服务器来处理, 这也是很大的开销.

  天真的做起来, 情况会更糟. 对于大部分的分期, 同一路径每一跳的数据包应该被发送到
相同的分析服务器中. 这个要求可以通过向一些洗牌器发送数据包来解决, 而后根据数据包
的头部hash值将数据包改组, 增加洗牌的服务器将会需要增加一倍的服务器.

**被动追踪的限制**. 在现实中, 被动的网络追踪可能不能提供解决问题的有效信息. 图一
显示了一种情况. TODO: 图介绍


### 3.2 Key ideas

 Everflow解决上述问题有四个关键点, 前三个是为了解决扩展性, 最后一个是未来解决
被动追踪的限制.

**在交换机上的匹配及镜像**. 商用交换机可以基于事先订立的规则来匹配, 然后执行某些
操作(镜像和封装), 而且这并不影响原有数据包的转发行为.Everflow使用这一点来减少
追踪开销. 特别的是, 我们设计了三种类型的匹配规则去解决DCN的一般错误. 这个规则绝不
是详尽的, 可以通过扩展规则来应对其他类型的错误.

  首先, 我们设计匹配规则去抓取每一个数据流, 最直接的方法是随机的抓取n个包中的一个,
这是严重偏向大流量的. 在DCN中, 数据流的大小分布是十分不均的, 这样就会错过了许多
小流量. 而那些小流量通常是面向用户的, 交互服务是有严格的性能要求的. 为了将小流量
覆盖到, 我们建立了一条新的规则, 这套新的规则基于TCP的SYN, FIN, RST数据包区域,
因为DCN的流量大部分由TCP表示, 这个规则允许我们追踪DCN中的每个TCP流量.

  而后, 我们配置额外的匹配规则可以灵活的追踪. 基础的TCP匹配不能抓取每一种类型的
错误, 例如, TCP流中部的数据包丢失. 例如, 我们可能想要追踪特定程序的数据包, 特定
的端口, 或是在某些服务器对之间. 为了允许可扩展的追踪, 我们允许数据包头部有一位被
标记为debug. 这个标记标准可以以任何方式定义, 只要全部的追踪影响低于一个阈值. 在
交换机中, 我们安装了一条规则去追踪任何有debug位的数据包, 就像软件开发者在编译期间
开启debug标志来启用细粒度的追踪, 因此, Everflow可以有效追踪常规数据包的任何子集.

  最后, 我们的追踪不只覆盖数据包. 一个DCN中有一小部分流量与网络协议相关, 例如
BGP, PFC, RDMA的流量包. 我们将其称为协议流量用以区分正常的数据流量. 尽管纯粹的
协议流量是很小的, 但却与DCN的健康相关. 因此, Everflow的规则中追踪所有协议流量.

**可扩展的跟踪分析器**. 尽管匹配规则限制了一些追踪开销, 但是因为DCN的规模庞大,
全部的追踪流量还是可能会很大. 为了减少这样的分析开销, 我们观察到, 在任何时候, 只有
一小部分被追踪的数据包(<0.01%)会出现异常行为(例如循环或丢弃). 这一现象的驱动我们
将正常包与异常包区别来对待, 例如, 前者保留详细记录每一跳的状态, 而后者的每个设备
状态, 我们在4.1章描述, 这样不同的对待使得我们减少了分析开销(三个数量级). 如果需要,
我们可以有选择的恢复丢失的信息, 只要通过探针追踪即可(如下所诉).

**基于交换机的洗牌器**. 就像在3.1中描述的一样, 我们需要一个开销很低的方式去将追踪
流量改组. 我们使用了先前所做的工作, 将商用交换机转变为一个硬件复用器来负载均衡.
首先在复用器中中定义一个虚拟VIP,  他将映射到一组`DIP`, 每个DIP将会对应一个分析
服务器. 我们之后将会改变所有的交换器配置, 使他们向VIP发送数据包, 当数据包`p`到达
复用器时, 复用器会根据5元组重定向数据包, 到特定的`DIP`中. 这就保证了具有相同
五元组的追踪数据包将会重定向到相同的DIP中.

  一个复用器可以利用交换机的能力来重定向流量(> 1Tbps). 这要比普通10Gbps的服务器
网卡快至少100倍, 并且极大的减少了重定向的开销, 而且可以通过增加相同VIP的复用器
来提高转换能力.

  我们需要注意封装后的数据包. (目前为止, 我们先忽略掉追踪数据包也被分装这一现象,
这个将会在6.1章讨论). 包封装经常会用在DCN的负载均衡以及网络虚拟化中. 图三TODO: 图问题
举了一个例子, 软件负载均衡器如可以打破追踪分析. 原始的数据包p的目的IP是 `VIP`(
复用器的IP), 一个复用器可以使用新的`DIP`(分析服务器)作为数据包p的新的目的IP.
这样做, 原始的数据包p和封装后的数据包p将会被发送至不同的分析器中, 也会被单独的处理.
为了避免这个错误, 我们安装了一个规则去匹配封装数据包的内层头部, 并且也配置复用器
对内层头部进行hash操作. 这允许我们追中数据包p的完整路径.

**有针对性的探测**. 像先前提出的那样, 丢包可能是由于多种原因发生的. 有时, 只进行
被动的追踪可能不会有效区分不同的可能性. 这样的歧义产生了如下的问题: 如果我们可以
任意的重放数据包路径呢? 更直接的是, 如果我们可以注入任何想要构造的数据包到任意的
交换机中, 并且追踪被注入的数据包(设置debug位)的行为, 我们称之位探针, 将会在6.2
章进行详细套路.

  一个有效应用就是根据采样或是聚合的数据恢复路径信息. 为了恢复任何数据包p的路径,
我们只需要将数据包p注入到第一跳的交换机中, 当然设置了debug位.

  进一步来讲, 探针克服了被动追踪时的限制. 在图1的例子中TODO: 图, 我们可以注入
许多数据包p的被封到交换机S2中, 去探测丢包是否是永久的. 另外, 我们也可以使用不同
的5元组模式来构造探针数据包, 以此来看看丢包是否是针对特定的5元组. 这样的探针不能
调试临时错误, 因为他们可能在探针初始化前就消失了. 我们认为这样是可以接受的, 因为
永久的错误要比临时错误更有影响.

  我们也扩展了探针, 使其不止能注入数据包至合适的交换机, 也能创造数据包并让其游历
我们期望的路径. 这个扩展使得我们可以衡量网络中任意链接的延迟. 就像在图2TODO: 图
一个探针p可被要求遍历S1->S2->S1, 因为p穿过了$S_{1}$两次, $S_{1}$就能产生两个
数据包p的时间$t_{1}$和$t_{2}$, 并且$t_{2} - t_{1}$就等于$S_{1}, $S_{2}$的
往返延迟.

  因为目前的许多交换机不提供时间戳函数, 我们不能直接的获取$t_{1}$, $t_{2}$.
然而, 两个数据包p在时间上是接近的, 并且也要经过相同的路径达到分析器. 因此, 我们
计算他们到达分析器的不同的时间之差约等于$t_{2} - t_{1}$.

## Trace collection and analysis

  现在我们退出了路径收集与分析的流水线, 就像在图4 TODO:图中显示的一样, 他由四个
主要组件组成: 控制器, 分析器, 存储器以及洗牌器. 在顶层, 有许多应用程序使用由
Everflow提供的信息去调试网络错误. 控制器负责协调其他组件以及和应用程序交流. 在
初始化过程中, 它配置交换机上的规则. 被匹配到的数据包将会镜像至洗牌器中, 而后会直接
到达分析器中, 而后将分析结果传递至存储器中. 控制器也提供了一些API, 允许Everflow
的应用程序去查询分析结果, 并且在主机上设置debug位. 我们将在这一章讨论分析器和
控制器, 在第六章讨论洗牌器和存储器.


### 4.1 Analyzers

  分析器是一群分布式的服务器, 每个服务器处理一部分追踪流量. 洗牌器将会均衡分析器
上的负载, 并且保证具有相同5元组的数据包会发送至相同的分析器中. 每个分析器保存了
两种类型的状态, packet trace 与 counter.

  **packet trace**. 分析器保存了一组追踪项的表格, 每个追踪项是一个链表, 保存了
相同来源的数据包,  每个追踪项由5元组和原始包的IPID进行标记, 也包含了每一跳的信息,
包括交换机的IP地址, 时间戳, TTL, 源MAC地址, 和`DSCP/ECN`, 我们认为1s内没有新
的数据包到达分析器, 一条trace就算结束(这比端到端延迟小得多)

  对于每一条完整的`packet trace`, 分析器只确认两件事, `loop`以及`drop`. 
一个`loop`表示相中的设备出现了多次. 一个`drop`表现为, 最后一跳的数据包与预期的
最后一跳不相同. 这可以由`DCN topology`与`routing policy`进行计算. 例如,
发往DCN内部IP地址的数据包期望的最后一跳是连接到IP地址的ToR交换机, 发往外部IP地址
的数据包最后一跳是DCN的边界交换机.

  为了正确处理由于软件负载均衡造成的数据包分装. 如果$p_{e}$的内部IP头的5元组和
IPID与$p_{o}$的头部相同, 我们合并原始的数据包$p_{o}$以及封装后的数据包$p_{e}$.

  尽管使用了"匹配镜像"机制, 数据包的总量还是很大. 为了减少存储器的开销, 每个分析器
写入存储器的只有异常行为(循环或丢包), 有debug位的探针, 或是重要协议(PFC和BGP).
对于其他追踪(大量而且普通), 每个分析器会定时(10s)来向存储器写入各种类型的计数器中.
最终`controller`会将analyzer中的数据聚合输出最终结果

 **Line load counters**. 对于每一个连接来说, 分析器将会通过追踪路径来计算
聚合负载(数据包数量, 字节以及流). 除了这个, 他也会计算更细粒度(fine-grained)
的负载. 比如拥有特定前缀的流量或是DCN内部的流量. 这些细粒度的计数器可以动态的通过
Everflow的程序来添加或是删除.

  **Latency counters**. 分析器将会计算使用探针的每条链接的延迟, 对于每个经过
负载均衡器的数据包, 他也会计算负载均衡器的延迟. 这一过程在图3(b)有描述, 图中的
均衡器连接了Tor交换机, 并且镜像原始的数据包以及封装后的数据包从相同的交换机到达了
相同的分析器, 我们可以使用到达时间来估计复用器的延迟. 这个延迟中包括了复用器与
交换机的往返延迟, 但是与复用器的延迟相比是微不足道的. 为了节省空间, 分析器将单个
延迟样本量化为延迟直方图中预定义的区域.

  **Mirrored packet drop counters**. 一个镜像数据包也可能在达到分析器时被
丢弃, 我们是可以这样推测的. 图5显示了数据包p从交换机$S_{1}$到交换机$S_{2}$.
然而, p的追踪结果只包括$S_{2}$, 缺少了$S_{1}$, 清楚的表明了从$S_{1}$到分析器
的数据包丢失了. 在我们目前的部署中, 这样的丢包率是很低的(大概0.001%).

  有时, 镜像包的丢失有可能是某个洗牌器或是分析器的拥塞. 为了提高流水线的可靠性,
我们部署了多个洗牌器, 多个分析器在DCN的不同位置. 将镜像流量转移出来, 从那些展现出
高丢包率的洗牌器或是分析器中.

### 4.2 Controller APIs

  Everflow应用程序通过API与控制器进行交互, 进行错误调试. 使用这些API, 应用程序
可以查找数据包路径, 安装细粒度的负载计数器, 触发探针, 以及在加入debug位后追踪路径.

  **GetTrace(Filter, Condition, StartTime, EndTime)**被用来获取在StartTime
和EndTime中的数据包路径. Filter参数指定了数据包类型, 就像在WireShark中使用的
筛选器一样. 它允许基于以太网络, IP, TCP或是UDP的头部来过滤数据包, 也允许检测
镜像数据包的外层头部(包括发送数据包的交换机IP). 例如Filter
`ip.proto==6 && ip.dst == 196.16.0.0/16 && switch == 10.10.0.10`
匹配所有的到`192.16.0.0/16`并且由交换机`10.10.0.10`所镜像的流量. Condition
参数指定了一些不能从数据包头部提取的属性. 例如, 它允许过滤一些是否包含环路或是丢包,
或是软件负载均衡器延迟过大的数据包.

  **GetCounter(Name, StartTime, Endtime)**被用来取回在StartTime到EndTime
中计数器的数值, 每个计数器被定义为一个名称, 就像"SwitchX_PortY_TCP".

  **AddCounter(Name, Filter) & RemoveCounter(Name)**被用来动态的添加或
删除细粒度的计数器, Filter就像上述的一样.

  **Probe(Format, Interval, Count)**被用来启动探针. 探针的结果可以稍后通过
`GetTract()`和`GetCounter()`获取到. Interval和Count参数制定了频率和探针发送
的总数. Format参数制定了探针的数据包, 包括3层和4层的数据包头部. 它与先前的Filter
参数类似, 只要稍做改变即可支持数据包封装. 例如
`ip.src == SIP1,SIP2 && ip.dst == DIP1,DIP2 && ip.proto == 6`定义了一个
由IP到IP的TCP数据包, 外层源IP是SIP1, 外层目的IP是DIP1, 内层源IP是SIP2, 内层
目的IP是DIP2.

  **EnableDbg(Servers, Filter) & DisableDbg(Servers, Filter)**被用来在
特定的服务器上标记或清除debug位. Filter参数与上面相同. 控制器只有在全部的数据流量
没有超过系统负载时才接受EnableDbg这样的请求.


## Everflow Applications

  使用Everflow的API来编写程序是直接的. 现在, 我们展示了几个示例程序, 它们帮助
我们解决了在第二章描述的问题.


  **延迟探查**. 许多DCN的服务, 类似于查找和分布式的内存缓存, 需要很低的延迟. 为了
找出任意一对服务器的延迟较高的原因. 延迟探查工具将会首先两个服务器TCP的SYN包中标记
debug位. 从这些数据包的追踪上, 可以得知在路径上的网络设备, 继而通过探针来衡量每一条
的延迟. 探针衡量每个链接的双向而不是单向的延迟. 这种本地化程度在实践中已经足够了.
通过本地化的信息, 探查器可以很快的辨识造成这个原因的网络设备.

  **包丢失调试器**. 丢包会严重的降低程序的性能, 导致很低的吞吐量, 延时或是不可达.
他们是相当难以调试的因为许多原因: 拥塞, 软件bug, 或是配置错误. 我们的丢包调试器
定期的检查数据包追踪来显示数据包丢失. 假设有一个数据包p的追踪路径, 基于p被捕获的
最后一跳, 调试器将会推测出下一跳$S_{n}$. 例如: $S_{n}$可以从p的最后一跳的交换机
输出接口或是DCN拓扑和路由中推断出来. 之后, 他将会注入探针至$S_{n}$检查丢包是否
是永久的, 或是丢包有特定的特征.

  **环路调试器**. 环路在DCN中不是很常见. 然而, 当他们发生时. 他们会不必要的浪费
资源或是连接. 我们的环路调试器观察数据包路径是否包括一个环. 当环路发生时, 首先注入
探针去看环路是否为永久的. 如果是永久的. 他将会向操作员报告一份环路上的设备名单,
操作员可以通过关闭其中一个设备来打破环路, 在这个过程中, 调试器会持续注入探针直到
环路消失.

  **ECMP探查器**. 在DCN中, 交换机经常使用ECMP去分割流量使其进入下一跳. 因此,
由于hash函数的偏向或是路由的问题, 负载可能并不均衡, 这会导致链路拥塞. 对于每个
交换机, ECMP探查器将会监控所有链接的总负载. 当检测到负载不均时, 他将通过更细粒度
的负载计数器来查看不均衡影响了全部流量或仅仅是一个子集(来自或到达特定的前缀).
这个分析结果帮助操作员快速的检查和定位错误.

  **基于RoCEv的RDMA调试器**. 基于RoCEv2的RDMA(远程直接内存访问) 是新兴的协议,
能达到很高的吞吐量, 并且极低的延迟, 而且不需要很多的CPU开销. 通过使用PFC来启用
无丢包以太网结构, 可以简化RDMA协议并将其卸载到NIC. 然而, 在我们的DCN中, 我们发现
RDMA有时不能达到其预想的性能, 由于NIC中的软件bug. 调试这样的问题是十分困难的,
由于NIC被第三方供应商提供, 我们只有有限的方式去指挥NIC中的RDMA代码.

  我们在Everflow中建立了一个RDMA调试器. 它追踪所有与RDMA相关的数据包, 例如PFC和
NACK. 控制数据包提供了一个可靠性且独立的方式, 不仅可以观察RDMA流的实际行为, 还可以
调试第三方供应商代码中的实现问题.

## Implementation

  整个Everflow系统由大致10k行C++代码完成, 5个Everflow程序大概由700行C#代码
完成. 下面, 我们省略了控制器的细节(其实现是相当直接的)和洗牌器(这与Duet中描述的
Mux类似), 我们专注于其他方面.

### 6.1 Switch configurations

  默认情况下, 我们在TCAM表中配置规则以匹配TCP的SYN/FIN/RST标志, 我们使用DSCP
字段中作为debug位, 使用IPID中的n位来表示$2^{n}$个数据包中的一个. 例如, 当我们
使用IPID中的10个bit位时, 我们可以表示1到1024个数据包. 因为每个交换机都有相同的
规则, 所有采样的分组在所有交换机上将保持一致. 对于封装过的数据包, 我们配置过滤
规则时通过匹配内部的TCP/IP头部信息, 以保证他能像原始包一样被发送到相同的分析器中.
最终, 我们配置规则以在以太网类型0x8808(包括PFC的L2控制数据包), TCP端口179(BGP
数据包)和RDMA NACK上匹配, 规则总数大约为20, 这只占TCAM表的一小部分.

  当数据包匹配到了任何规则, 交换机就会镜像它, 并且使用GRE
(Generic Routing Encapsulation)封装镜像包. 图6显示了GRE数据包的格式, 源IP
为交换机的回环IP, 目的IP为洗牌器的IP, 内容为原始的数据包(从L2层头部开始), 在
GRE头部之中, 有一个协议位表明这是Everflow镜像后的数据包. 我们也在每个交互机上配置
了黑名单, 防止对已经镜像后的数据包进行操作.

  "匹配镜像"完全的由交换机完成. 这一实现利用了交换机ASIC强大的处理能力而且并不影响
交换机的CPU.

### 6.2 Guided prober

  探针的关键函数就是注入任意需要的数据包到任意的交换机中. 它使用原始的套接字API来
制作任意数据包字段, 例如IP和L4(TCP, UDP, ICMP等等)的头部. 构造的数据包p设置了
debug位允许追踪, 并且在负载中携带了签名, 可以轻松的被Everflow的分析器所识别.
为了发送p到交换机S, 我们使用商用交换机具有的解封数据包的能力. 我们首先创造了探针
数据包$p^{`}$, 基于原始的S封装后的数据包p, 并将S的IP地址作为目的IP, 而后将其
发送出去. 我们也在S上配置了一条规则可以解封任何以S为目的IP的封装数据包. 因此,
根据接收到的$p^{`}$, 交换机S将会解封至数据包p, 而后, 像正常的转发逻辑一样将其
转发.

  事实上, 我们可以使用上述的$p^{`}$的创造方式来任意的扩展使得p可以到达我们想要的
路径. 这可以用来衡量链接($S_{1}, S_{2}$)的时延. 首先, 我们将p改造, 将$S_{1}$
作为目的IP, 之后再次使用$S_{2}$作为目的IP来封装, 而后再次使用$S_{1}$作为目的IP
来封装. 最终得到的数据包将会遵从$S_{1}->S_{2}->S_{1}$的路径来传播.

  为了防止探针数据包被服务器使用, 我们故意将其TCP或是UDP的校验和设置错误, 以防被
其他应用程序使用.


### 6.3 Analyzer

  分析器使用一个自定义的抓取库来抓取镜像数据包. 这个库支持RSS, 也就是允许使用多核
来接受数据包, 这个库对数据包中的来源与目的IP进行hash操作来分配CPU核心(如果是封装
后的数据包, 则使用内层的IP). 我们使用多个分析线程来提高吞吐量.


### 6.4 Storage

  Everflow的存储器是建立在SCOPE上的 -- 一个可扩展的分布式的数据处理系统. 在SCOPE
中, 数据被建模为有行类型列组成的表. 这些表可以被SQL类似的语言来处理, 也允许用户
定义的操作, 类似于extractor(解析和构建文件中的行), processors(逐行处理),
reducers(逐组处理), combiners(合并两个输入的行).

  我们存储数据包在一个多列的表格中, 其中每一行对应一个追踪记录. 列包括追踪记录的
三个部分, 首先是整个数据包的信息, 数据包头部和负载被放置在不同的列来简化处理. 第二
部分是每一跳的信息, 时间戳, TTL, 源MAC地址. 因为跳数是不固定的, 我们将每一跳的
信息储存在一列中, 最后一部分是路径数据的元数据, 包括路径长度, 是否为探针, 是否有环,
是否丢包, 这个数据可以由filter, condition, 还有time这几个参数来进行处理.

  我们也在表格中储存所有分析器产生的计数器. 表中的每一行代表一个从分析器取得的快照,
一行中也有分析器的ID和快照记录的时间. 为了返回计数器查询, 控制器会将根据时间和名字
匹配到的行的中的数据进行累加.

## Ddeployment and experience

  2014年, 我们在微软两个的数据中心上进行部署. 第一个部署的是前期制作的集群, 其中有
37台交换机. 第二个部署是对生产群中超过2500台交换机中的440台进行部署. 两个集群承载了
许多DC程序的流量. 除此之外, 我们也在特定的生产交换机上开启了Everflow, 用以支持
在线错误的调试. 目前, 我们扩展了Everflow的部署以使其可以被交换机和集群使用.
在下文中, 我们仍会分享我们使用Everflow去调试许多不同的数据中心错误.

### 7.1 Latency problem

  一个多层的搜索程序经常会抱怨在一些服务器中的延迟和抖动. 该应用程序分配严格的每层
延迟约1ms, 其中包括服务器的处理时间. 由于使用了ECMP路由, 有数百个链接可以导致这样
的问题. 对于操作员来说处理罪魁祸首的交换机或是链接十分的困难.

  一个可用的方案是使用traceroute去推测每条链路的延迟. 然而, traceroute衡量了
在探针服务器到交换机之间的往返时间, 这要被所有路上的链接或是反向路径所影响. 更重要
的是, traceroute的探针需要被交换机的CPU处理, 得到的RTT可能会被交换机的控制平面
影响, 而他是一直变化的. 因此, 使用traceroute衡量RTT可能会变得不稳定和噪声.



------------

## Related work

  我们的工作基于一些相类似的工作.

**Mirroring**. 就像Everflow, Planck 和PacketHistory通过交换机的镜像来监控
网络状态. 在Planck中, 交换机将订阅端口的流量镜像直接连接到服务器上. 它关注于单个
设备上的数据包, 而不是网络范围内的数据包跟踪. PacketHistory镜像所有交换机上的
数据包. 在庞大的DCN中, 这样的方法导致了显著的系统开销, 不论是传输还是处理. 他也
有可能造成拥塞并伤害运行着的服务. 相比之下, Everflow使用了"匹配机制", 选择合适
的流量去镜像.

**Sampling**. 最近的许多工作关注于采样网络流量. NetFlow和OpenSketch在数据流
层面汇总信息, 不提供数据包层面的细粒度的信息. sFlow只是随机的在交换机中选择数据
包. 它们只能提供每个设备的信息, 不能在多个设备中追踪数据包路径. 有其他的工作使用
固定的hash函数来进行采样.这不能控制采样的数据包, 以及如何分析采样数据包. 相反,
Everflow使用了基于规则的端对端的解决方案, 可以追踪有效信息 并且实时进行分析.

**Probing**. 每个数据包追踪工具类似traceroute, ICMP和TUlip可以追踪网络中的
数据包路径. 最近的一项工作提出使用IP预定义的时间戳选项来剖析RTT, 但是受限于Internet
中路径的特定部分. 这些工作由于使用的数据包与真实的程序数据包不同, 被视为外在的诊断.
相比之下Everflow就要内在一些.

  OFRewind使用"record and replay", 这是另一种方式的探针. Everflow拥有相近
的能力, 但是不仅仅能重复过往的路径, Everflow还可以构建探针用以测量交换机的延迟.

  不仅如此, 目前的工具不能追踪中间件以外的情况(例如负载均衡和虚拟网关), 这可能需要
封装或是修改数据包. Everflow关联了中间件的初入, 来构造整个数据包路径.

**Fault detection**. 许多工作已经通过检测路由配置来发现网络错误. 他们能够分析
路由规则并且确认没有网络规则合理(没有转发环). 它基于控制平面的信息关注于转发表是否
合理. 因此, 他们不能诊断数据包层面的错误. 为了错误探测, 其他工作为监控虚拟交换机,
或是使用运营数据, 类似于路由日志, 或是 AS路由传播. 他们均是Everflow的补充, 因为
他们都是关注于Everflow的不同方面, 而Everflow致力于提供细粒度的数据包层面的错误
检测.
